{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7014801-7e5c-43b2-9ef8-0b290cbd1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.hazard import Hazard, get_area, select_most_common\n",
    "\n",
    "with open(\"resources/annotations_public.pkl\", \"rb\") as f:\n",
    "    anns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2df2ad3-df80-4ac7-a5ed-2148b89207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Video -> Track -> Frame hierarchy.\n",
    "data = defaultdict(lambda: defaultdict(dict))\n",
    "for video, video_data in anns.items():\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for obj in frame_data[\"challenge_object\"]:\n",
    "            data[video][obj[\"track_id\"]][frame] = {\n",
    "                \"bbox\": obj[\"bbox\"],\n",
    "                \"area\": get_area(obj[\"bbox\"]),\n",
    "                \"frame\": frame,\n",
    "                \"video\": video,\n",
    "            }\n",
    "    data[video] = dict(data[video])\n",
    "data = dict(data)\n",
    "\n",
    "\n",
    "# Create hazard objects\n",
    "hazards = defaultdict(dict)\n",
    "for video, video_data in data.items():\n",
    "    for track, track_data in video_data.items():\n",
    "        hazards[video, track] = Hazard(video, track, track_data)\n",
    "\n",
    "\n",
    "# Add captions to hazards\n",
    "cap_largest = torch.load(\n",
    "    \"resources/molmo-captions/molmo-obj-cap-largest.pkl\", weights_only=False\n",
    ")\n",
    "for (video, track), hazard in hazards.items():\n",
    "    caps = [i.split() for i in cap_largest[video, track]]\n",
    "    caps = [item for column in zip(*caps) for item in column]  # Columnwise flatten\n",
    "    caps = np.array([i.lower() for i in caps])\n",
    "    caps_most_common = select_most_common(caps, k=5)\n",
    "    hazard.caption_list = caps_most_common\n",
    "\n",
    "hazards_remap = defaultdict(dict)\n",
    "for (video, track), hazard in hazards.items():\n",
    "    hazards_remap[video][track] = hazard\n",
    "hazards_remap = dict(hazards_remap)\n",
    "\n",
    "# Parse CIFAR Data\n",
    "obj_cls = torch.load(\"resources/cifar-classification/all-dense.pkl\", weights_only=False)\n",
    "cls_data = defaultdict(dict)\n",
    "for video, video_data in obj_cls.items():\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for i in frame_data:\n",
    "            cls_data[video, i[\"track_id\"]][frame] = {\n",
    "                \"top10_probs\": i[\"top10_probs\"],\n",
    "                \"top10_class\": i[\"top10_class\"],\n",
    "            }\n",
    "cls_data = dict(cls_data)\n",
    "\n",
    "\n",
    "# Add CIFAR class data to hazards\n",
    "for (video, track), hazard in hazards.items():\n",
    "    assert cls_data[video, track].keys() == hazards[video, track].frames.keys()\n",
    "    for frame, frame_data in hazards[video, track].frames.items():\n",
    "        frame_data[\"probs10\"] = cls_data[video, track][frame][\"top10_class\"]\n",
    "        frame_data[\"class10\"] = cls_data[video, track][frame][\"top10_probs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2869f-498a-4d5f-9b91-32a025b4f77d",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c8091f0-8638-4c19-991d-aa72341c7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:17<00:00, 11.40it/s]\n",
      "55770it [00:02, 25410.88it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sub = defaultdict(list)\n",
    "for video, video_hazard in tqdm(hazards_remap.items()):\n",
    "    for track, hazard in video_hazard.items():\n",
    "        frame_ids = list(hazard.frames.keys())\n",
    "        if hazard.dangerous:\n",
    "            for frame in frame_ids:\n",
    "                data_sub[f\"{video}_{frame}\"].append(\n",
    "                    {\"track\": int(track), \"name\": hazard.caption}\n",
    "                )\n",
    "data_sub = dict(data_sub)\n",
    "\n",
    "\n",
    "# Create submission\n",
    "df_template = pd.read_csv(\"./submissions/results_driverchange_ensemble.csv\")\n",
    "df_sub = []\n",
    "for i, row in tqdm(df_template.iterrows()):\n",
    "    row_dict = {\"ID\": row[\"ID\"], \"Driver_State_Changed\": row[\"Driver_State_Changed\"]}\n",
    "    if row[\"ID\"] in data_sub:\n",
    "        for i, data in enumerate(data_sub[row[\"ID\"]]):\n",
    "            row_dict[f\"Hazard_Track_{i}\"] = str(data[\"track\"])\n",
    "            row_dict[f\"Hazard_Name_{i}\"] = data[\"name\"]\n",
    "\n",
    "    df_sub.append(row_dict)\n",
    "df_sub = pd.DataFrame(df_sub).fillna(\" \")\n",
    "df_sub.to_csv(\"./submissions/final_submission-v0.1.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc2122-19a8-4467-97fc-febab8682691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
