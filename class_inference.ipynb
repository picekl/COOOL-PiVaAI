{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7014801-7e5c-43b2-9ef8-0b290cbd1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('../annotations_public.pkl', 'rb') as f:\n",
    "    anns = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93887f-c061-445d-b252-21d675d2edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_common(array, k):\n",
    "    unique, counts = np.unique(array, return_counts=True)\n",
    "    first_indices = np.array([np.where(array == u)[0][0] for u in unique])\n",
    "    sorted_indices = np.lexsort((-counts, first_indices))  # Negative counts for descending order\n",
    "    top_k = unique[sorted_indices[:k]]\n",
    "    return top_k.tolist()\n",
    "\n",
    "\n",
    "def get_area(bbox):\n",
    "    \"\"\"\n",
    "    Calculate the area of a bounding box.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Ensure valid bounding box\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        raise ValueError(\"Invalid bounding box dimensions.\")\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return width * height\n",
    "\n",
    "\n",
    "\n",
    "class Hazard:\n",
    "    def __init__(self, video, track, frames):\n",
    "        self.video = video\n",
    "        self.track = track\n",
    "        self.frames = frames\n",
    "        self.caption_list = []\n",
    "        self.caption_list_words = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Hazard {self.video} {self.track}\"\n",
    "\n",
    "    @property\n",
    "    def caption(self):\n",
    "        if len(self.caption_list) == 0:\n",
    "            return ' '\n",
    "        else:\n",
    "            return ' '.join(self.caption_list)\n",
    "    \n",
    "    @property\n",
    "    def dangerous(self):\n",
    "        #return hazard.caption_list[0] not in ['suv', 'vehicle', 'car', 'truck', 'bus', 'motorcycle', 'van', 'traffic']\n",
    "        return self.get_cifar_classes()[0] not in ['pickup_truck', 'bus', 'tank', 'motorcycle', 'cloud']\n",
    "\n",
    "    def visualize(self, frame_idx=None, folder_path=\"/home/cermavo3/projects/kaggle/coool/data\"):\n",
    "        if frame_idx is not None:\n",
    "            frame_id = list(self.frames.keys())[frame_idx]\n",
    "        else:\n",
    "            frame_id = pd.DataFrame(self.frames).T['area'].idxmax() # Largest by area\n",
    "    \n",
    "        cap = cv2.VideoCapture(f\"{folder_path}/{self.video}.mp4\")\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "    \n",
    "            # Process the target frame\n",
    "            if frame_count == frame_id:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "                # Draw bounding box\n",
    "                x1, y1, x2, y2 = np.array(self.frames[frame_id]['bbox']).round().astype(int)\n",
    "                image = cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "                \n",
    "                # Add track_id text in the upper-left corner\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 1.2\n",
    "                font_thickness = 2\n",
    "                text_size = cv2.getTextSize(self.caption, font, font_scale, font_thickness)[0]\n",
    "                \n",
    "                # Calculate text position (upper-left corner)\n",
    "                text_x = 10  # Fixed x-coordinate\n",
    "                text_y = text_size[1] + 10  # Add a small offset from the top\n",
    "    \n",
    "                # Draw text background for better visibility (optional)\n",
    "                text_bg_x2 = text_x + text_size[0]\n",
    "                text_bg_y2 = text_y + 5  # Add a small padding below the text\n",
    "                image = cv2.rectangle(image, (text_x - 5, text_y - text_size[1] - 5), \n",
    "                                       (text_bg_x2 + 5, text_bg_y2), color=(0, 255, 0), thickness=-1)\n",
    "                \n",
    "                # Draw the text on the image\n",
    "                image = cv2.putText(image, self.caption, (text_x, text_y), font, font_scale, (0, 0, 0), thickness=font_thickness)\n",
    "                break\n",
    "            frame_count += 1    \n",
    "        cap.release()\n",
    "        return image\n",
    "\n",
    "    def get_cifar_classes(self):\n",
    "        df = pd.DataFrame(self.frames).T\n",
    "        df_extended = pd.DataFrame({\n",
    "            'probs': np.concatenate(df['probs10'].values),\n",
    "            'class': np.concatenate(df['class10'].values),\n",
    "            'area': np.concatenate([np.repeat(v, 10) for v in df['area']]),\n",
    "        })\n",
    "        select = df_extended.groupby('class').apply(lambda g: (g['probs'] * g['area']).mean()).sort_values()\n",
    "        return select.idxmax(), select.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c53d7-b61b-465e-9011-02a66fb09d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df2ad3-df80-4ac7-a5ed-2148b89207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Video -> Track -> Frame hierarchy.\n",
    "data = defaultdict(lambda: defaultdict(dict))\n",
    "for video, video_data in anns.items():\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for obj in frame_data['challenge_object']:\n",
    "            data[video][obj['track_id']][frame] = {\n",
    "                'bbox': obj['bbox'],\n",
    "                'area': get_area(obj['bbox']),\n",
    "                'frame': frame,\n",
    "                'video': video,\n",
    "            }\n",
    "    data[video] = dict(data[video])\n",
    "data = dict(data)\n",
    "\n",
    "\n",
    "# Create hazard objects\n",
    "hazards = defaultdict(dict)\n",
    "for video, video_data in data.items():\n",
    "    for track, track_data in video_data.items():\n",
    "        hazards[video, track] = (Hazard(video, track, track_data))\n",
    "\n",
    "\n",
    "# Add captions to hazards\n",
    "cap_largest = torch.load('../results/molmo-obj-cap-largest.pkl', weights_only=False)\n",
    "for (video, track), hazard in hazards.items():\n",
    "    caps = [i.split() for i in cap_largest[video, track]]\n",
    "    caps = [item for column in zip(*caps) for item in column] # Columnwise flatten\n",
    "    caps = np.array([i.lower() for i in caps])\n",
    "    caps_most_common = select_most_common(caps, k=5)\n",
    "    hazard.caption_list = caps_most_common\n",
    "\n",
    "hazards_remap = defaultdict(dict)\n",
    "for (video, track), hazard in hazards.items():\n",
    "    hazards_remap[video][track] = hazard\n",
    "hazards_remap = dict(hazards_remap)\n",
    "\n",
    "# Parse CIFAR Data\n",
    "obj_cls = torch.load('../results/cifar-obj-class/all-dense.pkl', weights_only=False)\n",
    "cls_data = defaultdict(dict)\n",
    "for video, video_data in obj_cls.items():\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for i in frame_data:\n",
    "            cls_data[video, i['track_id']][frame] = {\n",
    "                'top10_probs': i['top10_probs'],\n",
    "                'top10_class': i['top10_class'],\n",
    "            }\n",
    "cls_data = dict(cls_data)\n",
    "\n",
    "\n",
    "# Add CIFAR class data to hazards\n",
    "for (video, track), hazard in hazards.items():\n",
    "    assert cls_data[video, track].keys() == hazards[video, track].frames.keys()\n",
    "    for frame, frame_data in hazards[video, track].frames.items():\n",
    "        frame_data['probs10'] = cls_data[video, track][frame]['top10_class']\n",
    "        frame_data['class10'] = cls_data[video, track][frame]['top10_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb311b-c2a2-4fef-85a6-32b07db05dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:41<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_nouns(sentence, nlp):\n",
    "    doc = nlp(sentence)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return nouns\n",
    "\n",
    "blacklist = set([\n",
    "    'road', 'collision', 'risk', 'middle', 'brake', 'light', 'traffic', 'street', 'crossing', 'lane',\n",
    "    'danger', 'caution', 'issue', 'lights', 'door', 'accident', 'crosswalk', 'end', 'branch', 'hazard',\n",
    "    'failure', 'sidewalk', 'line','fire', 'crash', 'edge', 'headlights', 'load', 'intersection', 'blocks',\n",
    "    'work', 'safety', 'visibility', 'parking', 'roadway', 'view', 'side', 'birds', 'brakes', 'path',\n",
    "    'distance', 'swerving', 'police', 'front', 'ditch', 'conditions', 'driving', 'lot', 'kneeling', 'surface',\n",
    "    'dent', 'separates', 'wild', 'wildlife',\n",
    "    'driveway', 'highway', 'entrance', 'control', 'scene', 'opening', 'quality', 'crack', 'drivers', 'driver',\n",
    "    'glare', 'curves', 'obcuring', 'closure', 'explosion', 'riding', 'ground', 'flow', 'leg', 'roof',\n",
    "    'imminent', 'officer', 'headlight', 'lines', 'speed', 'plane', 'clothing', 'costume', 'canada',\n",
    "    'car', 'vehicle', 'truck', 'bus', 'cars',\n",
    "])\n",
    "\n",
    "\n",
    "caps_words_data = torch.load('../results/molmo-obj-caption-short-square/all.pkl', weights_only=False)\n",
    "caps_word = defaultdict(dict)\n",
    "for video, video_data in tqdm(caps_words_data.items()):\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for i in frame_data:\n",
    "            nouns = extract_nouns(i['caption'], nlp)\n",
    "            nouns = [i.lower() for i in nouns]\n",
    "            nouns = [i for i in nouns if i not in blacklist]\n",
    "            caps_word[video, i['track_id']][frame] = nouns\n",
    "caps_word = dict(caps_word)\n",
    "\n",
    "\n",
    "# Add new captions to hazards\n",
    "for (video, track), hazard in hazards.items():\n",
    "    frames = pd.DataFrame(hazard.frames).T.sort_values(by='area').tail(50)['frame'].values\n",
    "    words = []\n",
    "    if (video, track) in caps_word:\n",
    "        for frame in frames:\n",
    "            if frame in caps_word[video, track]:\n",
    "                words.extend(caps_word[video, track][frame])\n",
    "        hazard.caption_list_words = pd.Series(words).value_counts().head(10).index.to_list()\n",
    "    else:\n",
    "        hazard.caption_list_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8fe9d-8009-46c4-ae94-6422164e58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041bcb3-eb6c-4871-9c12-32973a41e10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f61331-96ef-4ea2-81a2-0c3a621e713c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebf2869f-498a-4d5f-9b91-32a025b4f77d",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae45f7-0192-4fa3-8270-660c3c9491a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get\n",
    "def get_new_captions(hazard):\n",
    "    caption_nouns = [i for i in hazard.caption_list if i not in blacklist]\n",
    "    \n",
    "    new_list = caption_nouns[:3]\n",
    "    new_list = new_list + [i for i in hazard.caption_list_words[:10] if i not in new_list]\n",
    "\n",
    "    if new_list == []:\n",
    "        return ' '\n",
    "    else:\n",
    "        return ' '.join(new_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8091f0-8638-4c19-991d-aa72341c7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:16<00:00, 11.93it/s]\n",
      "55770it [00:02, 27592.61it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sub = defaultdict(list)\n",
    "for video, video_hazard in tqdm(hazards_remap.items()):\n",
    "    for track, hazard in video_hazard.items():\n",
    "        frame_ids = list(hazard.frames.keys())\n",
    "        if hazard.dangerous:\n",
    "            for frame in frame_ids:\n",
    "                data_sub[f\"{video}_{frame}\"].append({'track': int(track), 'name': hazard.caption})\n",
    "                #data_sub[f\"{video}_{frame}\"].append({'track': int(track), 'name': get_new_captions(hazard)})\n",
    "data_sub = dict(data_sub)\n",
    "\n",
    "\n",
    "#Create submission\n",
    "df_template = pd.read_csv('/home/cermavo3/projects/kaggle/coool/submissions/results_17122024_driver_of-bboxsize-ensemble-bkpt4.csv')\n",
    "df_sub = []\n",
    "for i, row in tqdm(df_template.iterrows()):\n",
    "    row_dict = {\n",
    "        'ID': row['ID'],\n",
    "        'Driver_State_Changed': row['Driver_State_Changed']\n",
    "   }\n",
    "    if row['ID'] in data_sub:\n",
    "        for i, data in enumerate(data_sub[row['ID']]):\n",
    "            row_dict[ f'Hazard_Track_{i}'] = str(data['track'])\n",
    "            row_dict[ f'Hazard_Name_{i}'] = data['name']\n",
    "\n",
    "    df_sub.append(row_dict)\n",
    "df_sub = pd.DataFrame(df_sub).fillna(' ')\n",
    "df_sub.to_csv('../submissions/results_18122024_driver_best_alltracks_cifar-filter-molmo-newcaps3.csv', index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc2122-19a8-4467-97fc-febab8682691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-wt",
   "language": "python",
   "name": "venv-3dfauna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
