{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac710a81e1fe5df",
   "metadata": {},
   "source": [
    "# Depth estimation with Depth-Anything-V2-Large\n",
    "\n",
    "Depth Anything V2 is trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation (MDE) model with the following features:\n",
    "- more fine-grained details than Depth Anything V1\n",
    "- more robust than Depth Anything V1 and SD-based models (e.g., Marigold, Geowizard)\n",
    "- more efficient (10x faster) and more lightweight than SD-based models\n",
    "- impressive fine-tuned performance with our pre-trained models\n",
    "\n",
    "## Installation\n",
    "\n",
    "In this folder run the following commands.\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/spaces/depth-anything/Depth-Anything-V2\n",
    "cp -r Depth-Anything-V2/depth_anything_v2 .\n",
    "pip install -r Depth-Anything-V2/requirements.txt\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Download the [model](https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true) first and put it under the `checkpoints` directory.\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "model = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
    "model.load_state_dict(torch.load('checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "raw_img = cv2.imread('your/image/path')\n",
    "depth = model.infer_image(raw_img) # HxW raw depth map\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**For more please refer to [the official instructions](https://huggingface.co/depth-anything/Depth-Anything-V2-Large).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc616f-5e08-4bdf-86ad-ec1d9cd2c704",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:39.540641Z",
     "start_time": "2024-12-20T13:42:39.527757Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "54a598f462765e1c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:42.418862Z",
     "start_time": "2024-12-20T13:42:39.604384Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Large\n",
    "model = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
    "model.load_state_dict(torch.load('checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\n",
    "model.eval().to(device)\n",
    "\n",
    "print(\"Model loaded!\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65078/3459003255.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "c7dfdc4ca728b142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:42.500346Z",
     "start_time": "2024-12-20T13:42:42.489103Z"
    }
   },
   "source": [
    "ANNOTATIONS_PATH = \"../../resources/annotations_public.pkl\"\n",
    "VIDEOS_ROOT = \"/home/marek/datasets/coool-benchmark\"          # <---- UPDATE THIS ONE\n",
    "RESULTS_FOLDER = \"../../resources/depth-estimation\"  # Folder to save depth captures\n",
    "\n",
    "if not osp.exists(RESULTS_FOLDER):\n",
    "    os.makedirs(RESULTS_FOLDER)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:42.814502Z",
     "start_time": "2024-12-20T13:42:42.532734Z"
    }
   },
   "cell_type": "code",
   "source": "!pwd",
   "id": "b4adf2565b6b77f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marek/Work/COOOL_benchmark/COOOL-PiVaAI/video-processing/depth-estimation\r\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:42.836715Z",
     "start_time": "2024-12-20T13:42:42.821709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(file_path: str) -> dict:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ],
   "id": "fde6ccde7b488655",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "fbab95703685b793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T13:42:48.619713Z",
     "start_time": "2024-12-20T13:42:42.869611Z"
    }
   },
   "source": [
    "annotations = load_pickle(ANNOTATIONS_PATH)\n",
    "\n",
    "# Check if video folder exists\n",
    "if not osp.exists(VIDEOS_ROOT):\n",
    "    raise FileNotFoundError(f\"Videos folder does not exist: {VIDEOS_ROOT}\")\n",
    "\n",
    "video_names = sorted(list(annotations.keys()))\n",
    "if not video_names:\n",
    "    raise ValueError(\"No videos found in the annotations.\")\n",
    "\n",
    "# Process each video\n",
    "for video_name in tqdm(video_names, total=len(video_names)):\n",
    "    video_path = osp.join(VIDEOS_ROOT, f\"{video_name}.mp4\")\n",
    "\n",
    "    if not osp.exists(video_path):\n",
    "        print(f\"Warning: Video file not found: {video_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    video_data = []\n",
    "    video_stream = cv2.VideoCapture(video_path)\n",
    "    fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if not video_stream.isOpened():\n",
    "        print(f\"Error: Video {video_name} could not be opened. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    frame = 0\n",
    "    while video_stream.isOpened():\n",
    "        ret, frame_image = video_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process frame at 1-second intervals (assuming fps is not zero)\n",
    "        if fps > 0 and frame % int(fps) == 0:\n",
    "            try:\n",
    "                depth = model.infer_image(frame_image)  # Assuming model is defined elsewhere\n",
    "                img_path = osp.join(RESULTS_FOLDER, f\"{video_name}_{frame}.jpeg\")\n",
    "                cv2.imwrite(img_path, depth)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame} of video {video_name}: {e}\")\n",
    "\n",
    "        frame += 1\n",
    "\n",
    "    video_stream.release()\n",
    "\n",
    "print(\"Processing complete.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:05<18:11,  5.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m video_stream\u001B[38;5;241m.\u001B[39misOpened():\n\u001B[0;32m---> 29\u001B[0m     ret, frame_image \u001B[38;5;241m=\u001B[39m \u001B[43mvideo_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[1;32m     31\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
