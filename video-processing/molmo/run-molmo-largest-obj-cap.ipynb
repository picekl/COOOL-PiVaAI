{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c4041ee-e80e-45fd-8dad-26f8859f91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_text(image, prompt, processor, model):\n",
    "    img = Image.fromarray(image)\n",
    "    inputs = processor.process(\n",
    "        images=[img],\n",
    "        text=prompt\n",
    "    )\n",
    "    \n",
    "    # move inputs to the correct device and make a batch of size 1\n",
    "    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "    output = model.generate_from_batch(\n",
    "      inputs,\n",
    "      GenerationConfig(max_new_tokens=400, stop_strings=\"<|endoftext|>\"),\n",
    "      tokenizer=processor.tokenizer\n",
    "    )\n",
    "\n",
    "    # only get generated tokens; decode them to text\n",
    "    generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def crop_square_with_context(image, bounding_box, context_percent=0.0, min_size=10):\n",
    "    \"\"\"\n",
    "    Crops a square region from the image defined by a bounding box, ensuring minimum size\n",
    "    and adding additional context as a percentage of box size.\n",
    "    \"\"\"\n",
    "    bounding_box = np.array(bounding_box).round().astype(int)\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "\n",
    "    # Ensure minimum box size\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "\n",
    "    if box_width < min_size:\n",
    "        padding_x = (min_size - box_width) // 2\n",
    "        x1 -= padding_x\n",
    "        x2 += padding_x\n",
    "\n",
    "    if box_height < min_size:\n",
    "        padding_y = (min_size - box_height) // 2\n",
    "        y1 -= padding_y\n",
    "        y2 += padding_y\n",
    "\n",
    "    # Recalculate box dimensions\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "\n",
    "    # Ensure the box is square by making both sides equal to the larger dimension\n",
    "    side_length = max(box_width, box_height)\n",
    "\n",
    "    # Center the square box around the original bounding box\n",
    "    x_center = (x1 + x2) // 2\n",
    "    y_center = (y1 + y2) // 2\n",
    "\n",
    "    x1 = x_center - side_length // 2\n",
    "    x2 = x_center + side_length // 2\n",
    "    y1 = y_center - side_length // 2\n",
    "    y2 = y_center + side_length // 2\n",
    "\n",
    "    # Add context as a percentage of the side length\n",
    "    context = int(side_length * context_percent)\n",
    "    x1 -= context\n",
    "    y1 -= context\n",
    "    x2 += context\n",
    "    y2 += context\n",
    "\n",
    "    # Ensure the box stays within image bounds\n",
    "    height, width = image.shape[:2]\n",
    "    x1 = max(0, x1)\n",
    "    y1 = max(0, y1)\n",
    "    x2 = min(width, x2)\n",
    "    y2 = min(height, y2)\n",
    "\n",
    "    # Crop the square region\n",
    "    cropped_image = image[y1:y2, x1:x2]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "\n",
    "def get_img(filename, frame_id, folder_path=\"../data\"):\n",
    "    cap = cv2.VideoCapture(os.path.join(folder_path, filename))\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process the target frame\n",
    "        if frame_count == frame_id:\n",
    "            video = filename.split('.')[0]\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            break\n",
    "        frame_count += 1    \n",
    "    cap.release()\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_area(bbox):\n",
    "    \"\"\"\n",
    "    Calculate the area of a bounding box.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Ensure valid bounding box\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        raise ValueError(\"Invalid bounding box dimensions.\")\n",
    "\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    return width * height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f4279-dcd8-4772-aead-8f8d9e81ae2e",
   "metadata": {},
   "source": [
    "## Load MOLMO and Ground Truth annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68991f-86be-4496-a160-0342cd6aded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "# Load annotations\n",
    "with open('../annotations_public.pkl', 'rb') as f:\n",
    "    anns = pickle.load(f)\n",
    "\n",
    "# Process annotation to get area\n",
    "data = []\n",
    "for video, video_data in anns.items():\n",
    "    for frame, frame_data in video_data.items():\n",
    "        for track in frame_data['challenge_object']:\n",
    "            data.append({\n",
    "                'video': video,\n",
    "                'frame': frame,\n",
    "                'track_id': track['track_id'],\n",
    "                'bbox': track['bbox'],\n",
    "                'area': get_area(track['bbox']),\n",
    "            })\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa47006-6d5c-4ec2-8f2f-80cd34e30fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5073d8c7-6334-4989-bab0-9536bad86dd1",
   "metadata": {},
   "source": [
    "\n",
    "## Run MOLMO\n",
    "- For each object in each video:\n",
    "    - Do inference for top 5 largest boxes in video\n",
    "    - Crop square to prevent distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df679a-ea9f-4c44-a934-e1a3bbbf3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "    Propose 5 most likely class labels of the object, context of the image is traffic and unusual hazards such as various animals on the road. Write only the class names separated by spaces.\n",
    "    '''\n",
    "\n",
    "data = defaultdict(list)\n",
    "for (video, track_id), group in tqdm(df.groupby(['video', 'track_id'])):\n",
    "    select = group.sort_values(by='area', ascending=False).head(5)\n",
    "    for _, row in select.iterrows():\n",
    "        img = get_img(f'{video}.mp4', frame_id=row['frame'])\n",
    "        img_crop = crop_square_with_context(img, row['bbox'], 0.0)\n",
    "        text = get_text(img_crop, prompt, processor, model)\n",
    "        text = text.strip(' ').replace('\\n', ' ')\n",
    "        data[video, track_id].append(text)\n",
    "\n",
    "data = dict(data)\n",
    "torch.save(data, f'results/molmo-obj-cap-largest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c65e4-3bee-4267-aa13-34d51e523632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426d85e-2811-47c7-8689-9affbe492b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb31d9f-e975-4860-ac59-da922a8f961b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llms",
   "language": "python",
   "name": "venv-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
