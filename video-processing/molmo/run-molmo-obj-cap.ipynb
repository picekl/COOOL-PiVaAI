{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c82306-aeeb-470a-bfe9-4671ebc1316f",
   "metadata": {},
   "source": [
    "## MOLMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9276da82-b8f7-43cc-a02c-0b44fcaa6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "path = \"/home/cermavo3/projects/kaggle/coool/COOOL-Benchmark\"\n",
    "\n",
    "\n",
    "def extract_points(molmo_output, image):\n",
    "    image_w = image.shape[1]\n",
    "    image_h = image.shape[0]\n",
    "    points = []\n",
    "    for match in re.finditer(r'x\\d*=\"\\s*([0-9]+(?:\\.[0-9]+)?)\"\\s+y\\d*=\"\\s*([0-9]+(?:\\.[0-9]+)?)\"', molmo_output):\n",
    "        try:\n",
    "            point = [float(match.group(i)) for i in range(1, 3)]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            point = np.array(point)\n",
    "            if np.max(point) > 100:\n",
    "                # Treat as an invalid output\n",
    "                continue\n",
    "            point /= 100.0\n",
    "            point = point * np.array([image_w, image_h])\n",
    "            points.append(point)\n",
    "\n",
    "    if len(points) > 0:\n",
    "        points = np.stack(points)\n",
    "        points = points.round().astype(int)\n",
    "    else:\n",
    "        points = np.array([]).astype(int)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_text(image, prompt, processor, model):\n",
    "    img = Image.fromarray(image)\n",
    "    inputs = processor.process(\n",
    "        images=[img],\n",
    "        text=prompt\n",
    "    )\n",
    "    \n",
    "    # move inputs to the correct device and make a batch of size 1\n",
    "    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    #with torch.autocast(device_type=\"cuda\", enabled=True, dtype=torch.bfloat16):\n",
    "    output = model.generate_from_batch(\n",
    "      inputs,\n",
    "      GenerationConfig(max_new_tokens=400, stop_strings=\"<|endoftext|>\"),\n",
    "      tokenizer=processor.tokenizer\n",
    "    )\n",
    "    # only get generated tokens; decode them to text\n",
    "    generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def visualise_boxes(image, objects):\n",
    "    image = image.copy()\n",
    "    for obj in objects:\n",
    "        x1, y1, x2, y2 = np.array(obj['bbox']).round().astype(int)\n",
    "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "    img = Image.fromarray(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_points(image, points):\n",
    "    image = image.copy()\n",
    "    for point in points:\n",
    "        image = cv2.circle(image, point, radius=5, color=(0, 255, 0), thickness=-1)\n",
    "    img = Image.fromarray(image)\n",
    "    return img\n",
    "\n",
    "\n",
    "def crop_with_context(image, bounding_box, context_percent, min_size=20):\n",
    "    \"\"\"\n",
    "    Crops a region from the image defined by a bounding box, ensuring minimum size\n",
    "    and adding additional context as a percentage of box size.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy array): The original image.\n",
    "        bounding_box (tuple): The bounding box (x1, y1, x2, y2).\n",
    "        min_size (int): Minimum size for the width and height of the box.\n",
    "        context_percent (float): Percentage of box size to add as context (e.g., 0.1 for 10%).\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Cropped image region.\n",
    "    \"\"\"\n",
    "    bounding_box = np.array(bounding_box).round().astype(int)\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "    \n",
    "    # Ensure minimum box size\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "    \n",
    "    if box_width < min_size:\n",
    "        padding_x = (min_size - box_width) // 2\n",
    "        x1 -= padding_x\n",
    "        x2 += padding_x\n",
    "    \n",
    "    if box_height < min_size:\n",
    "        padding_y = (min_size - box_height) // 2\n",
    "        y1 -= padding_y\n",
    "        y2 += padding_y\n",
    "\n",
    "    # Recalculate box dimensions\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "\n",
    "    # Compute context in pixels\n",
    "    context_x = int(box_width * context_percent)\n",
    "    context_y = int(box_height * context_percent)\n",
    "    \n",
    "    # Add context, ensuring we stay within image bounds\n",
    "    height, width = image.shape[:2]\n",
    "    x1 = max(0, x1 - context_x)\n",
    "    y1 = max(0, y1 - context_y)\n",
    "    x2 = min(width, x2 + context_x)\n",
    "    y2 = min(height, y2 + context_y)\n",
    "    \n",
    "    # Crop the region\n",
    "    cropped_image = image[y1:y2, x1:x2]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def crop_square_with_context(image, bounding_box, context_percent, min_size=10):\n",
    "    \"\"\"\n",
    "    Crops a square region from the image defined by a bounding box, ensuring minimum size\n",
    "    and adding additional context as a percentage of box size.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy array): The original image.\n",
    "        bounding_box (tuple): The bounding box (x1, y1, x2, y2).\n",
    "        context_percent (float): Percentage of box size to add as context (e.g., 0.1 for 10%).\n",
    "        min_size (int): Minimum size for the width and height of the box.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Cropped square image region.\n",
    "    \"\"\"\n",
    "    bounding_box = np.array(bounding_box).round().astype(int)\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "\n",
    "    # Ensure minimum box size\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "\n",
    "    if box_width < min_size:\n",
    "        padding_x = (min_size - box_width) // 2\n",
    "        x1 -= padding_x\n",
    "        x2 += padding_x\n",
    "\n",
    "    if box_height < min_size:\n",
    "        padding_y = (min_size - box_height) // 2\n",
    "        y1 -= padding_y\n",
    "        y2 += padding_y\n",
    "\n",
    "    # Recalculate box dimensions\n",
    "    box_width = x2 - x1\n",
    "    box_height = y2 - y1\n",
    "\n",
    "    # Ensure the box is square by making both sides equal to the larger dimension\n",
    "    side_length = max(box_width, box_height)\n",
    "\n",
    "    # Center the square box around the original bounding box\n",
    "    x_center = (x1 + x2) // 2\n",
    "    y_center = (y1 + y2) // 2\n",
    "\n",
    "    x1 = x_center - side_length // 2\n",
    "    x2 = x_center + side_length // 2\n",
    "    y1 = y_center - side_length // 2\n",
    "    y2 = y_center + side_length // 2\n",
    "\n",
    "    # Add context as a percentage of the side length\n",
    "    context = int(side_length * context_percent)\n",
    "    x1 -= context\n",
    "    y1 -= context\n",
    "    x2 += context\n",
    "    y2 += context\n",
    "\n",
    "    # Ensure the box stays within image bounds\n",
    "    height, width = image.shape[:2]\n",
    "    x1 = max(0, x1)\n",
    "    y1 = max(0, y1)\n",
    "    x2 = min(width, x2)\n",
    "    y2 = min(height, y2)\n",
    "\n",
    "    # Crop the square region\n",
    "    cropped_image = image[y1:y2, x1:x2]\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "\n",
    "def get_img(filename, frame_id, anns=None, folder_path = \"/home/cermavo3/projects/kaggle/coool/data\"):\n",
    "    cap = cv2.VideoCapture(os.path.join(folder_path, filename))\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        # Process every Nth frame to match the target FPS\n",
    "        if frame_count  == frame_id:\n",
    "            video = filename.split('.')[0]\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if anns is not None:\n",
    "                objects = anns[video][frame_count]['challenge_object']\n",
    "                for obj in objects:\n",
    "                    x1, y1, x2, y2 = np.array(obj['bbox']).round().astype(int)\n",
    "                    image = cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "            break\n",
    "        frame_count += 1    \n",
    "    cap.release()\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cb44f-dc2d-4269-974b-b02b95069190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Open annotations\n",
    "with open('COOOL_benchmark/annotations_public.pkl', 'rb') as f:\n",
    "    anns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51297430-a568-4f06-9549-467f27d9f7ef",
   "metadata": {},
   "source": [
    "# Caption each object - Short square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98782d76-96d8-4182-b5a4-65f82cabec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/201 [00:50<2:47:06, 50.13s/it]/home/cermavo3/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-D-0924/1721478b71306fb7dc671176d5c204dc7a4d27d7/image_preprocessing_molmo.py:119: RuntimeWarning: divide by zero encountered in divide\n",
      "  required_scale_d = candidate_resolutions.astype(np.float32) / original_size\n",
      "100%|██████████| 201/201 [7:01:32<00:00, 125.84s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process(image, objects):\n",
    "    for obj in objects:\n",
    "        img_cropped = crop_square_with_context(image, obj['bbox'], 0.0)\n",
    "        prompt = 'Considering the context of traffic, caption the hazard in one short sentence of maximum 30 characters and 6 words.'\n",
    "        obj['caption'] = get_text(img_cropped, prompt, processor, model).strip(' ')\n",
    "    return objects\n",
    "\n",
    "\n",
    "folder_path = \"/home/cermavo3/projects/kaggle/coool/data\"\n",
    "target_fps = 5\n",
    "\n",
    "results = {}\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        video = filename.split('.')[0]\n",
    "        cap = cv2.VideoCapture(os.path.join(folder_path, filename))\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_skip = int(original_fps / target_fps)\n",
    "\n",
    "        # Initialize storage for this video\n",
    "        video_results = {}\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process every Nth frame to match the target FPS\n",
    "            if frame_count % frame_skip == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                objects = anns[video][frame_count]['challenge_object']\n",
    "                video_results[frame_count] = process(image, objects)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        results[video] = video_results\n",
    "torch.save(results, f'results/molmo-obj-caption-short-square/all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e3633-e395-4f62-b6fc-03fd8ba3b923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f546a95-fb95-47ba-a055-6d44e17c7267",
   "metadata": {},
   "source": [
    "# Caption each object - 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb482221-8945-47be-a462-9ff8d47712ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [8:40:01<00:00, 155.23s/it]   \n"
     ]
    }
   ],
   "source": [
    "def process(image, objects):\n",
    "    for obj in objects:\n",
    "        img_cropped = crop_square_with_context(image, obj['bbox'], 0.0)\n",
    "        prompt = '''\n",
    "        Propose 5 most likely class labels of the object, context of the image is traffic and unusual hazards such as various animals on the road. Write only the class names separated by spaces.\n",
    "        '''\n",
    "        obj['caption'] = get_text(img_cropped, prompt, processor, model).strip(' ')\n",
    "    return objects\n",
    "\n",
    "\n",
    "folder_path = \"/home/cermavo3/projects/kaggle/coool/data\"\n",
    "target_fps = 5\n",
    "\n",
    "results = {}\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        video = filename.split('.')[0]\n",
    "        cap = cv2.VideoCapture(os.path.join(folder_path, filename))\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_skip = int(original_fps / target_fps)\n",
    "\n",
    "        # Initialize storage for this video\n",
    "        video_results = {}\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process every Nth frame to match the target FPS\n",
    "            if frame_count % frame_skip == 0:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                objects = anns[video][frame_count]['challenge_object']\n",
    "                video_results[frame_count] = process(image, objects)\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        torch.save(video_results, f'results/molmo-obj-caption-words/{filename}.pkl')\n",
    "        results[video] = video_results\n",
    "torch.save(results, f'results/molmo-obj-caption-words/all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ce3c3-06b2-4c1d-a086-b520cc508ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c9671-a45b-4b69-9e20-b9aa400d9d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8469d7-1fd1-4ad4-9d10-164547d5b41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cd751-4eb4-4f72-b211-e673f1e9c7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llms",
   "language": "python",
   "name": "venv-llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
